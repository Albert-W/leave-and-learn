# 信息熵是什么？



1948年，C.E. Shannon 在《[A Mathematical Theory of Communication](http://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf)》中第一次提出了信息熵。

他在文章中将Entropy（信息熵） 与 Uncertainty（不确定度）等价使用的。

可以认为Entropy 就是 数学语言下的 Uncertainty。

他的原文对信息熵的属性进行了细致的分析，通过认识这些属性，可以帮助直观理解信息熵。



以下是我的节选简译:

信息熵是信息的不确定性（Uncertainty）的度量，不确定性越大，信息熵越大。

**一条信息消除的不确定性越大（即熵减越大），它蕴含的信息量越大。**

Shannon认为信息熵应有三大基本属性：

1. 信息熵应该是随概率连续变化的。
2. 当所有可能选择的概率Pi相等时（即1/n）,信息熵随总数n的增加而单调增加。即事件的可能选择越多，不确定性越大。
3. 当一个选择，可以分解为两个连续选择时。分解前后的熵值应该相等，不确定性相同。

基于上述条件，对于事件E, 每一个选择可能的概率为p1,p2,...,pn , 事件的信息熵为HE 。他给出信息熵的公式：

<a href="https://www.codecogs.com/eqnedit.php?latex=H_E=-\sum&space;_{i=1}^{n}p_ilog(p_i)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?H_E=-\sum&space;_{i=1}^{n}p_ilog(p_i)" title="H_E=-\sum _{i=1}^{n}p_ilog(p_i)" /></a>

特别情况下，当事件为互斥事件时，其中结果1的概率为p,结果2的概率为1-p, 则信息熵随p变化的函数图像为：


互斥事件的信息熵值
根据信息熵的定义及以上图示可知：

1. 在肯定(p=1) 与否定（p=0) 的条件下，不确定度为0，熵值为0；

2. 在p=1/n（互斥事件中为0.5）时，不确定度最大，熵值最大。

3. <img src="https://latex.codecogs.com/gif.latex?H(x,y)\leq&space;H(x)&plus;H(y)" title="H(x,y)\leq H(x)+H(y)" />


即一个联合事件的不确定度小于等于两个独立的事件的不确定度之合。

4. 任何使p1,p2,...,pn的概率趋向于均等的改变，都会增加熵值。

5. <img src="https://latex.codecogs.com/gif.latex?H(x,y)=&space;H(x)&plus;H_x(y)" title="H(x,y)= H(x)+H_x(y)" />


即联合事件x,y的不确定度等于x的不确定度加上在x已知的情况下，y的不确定度。

6. <img src="https://latex.codecogs.com/gif.latex?H(y)&space;\geq&space;H_x(y)" title="H(y) \geq H_x(y)" />


即知道x的信息，一定不会增加y的不确定度。当x,y互为独立事件时，y的不确定度保持不变。

以上是香农对信息熵的基本属性描述，欢迎批评指正。